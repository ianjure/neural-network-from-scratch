{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wiyuIPb31pcK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ianjure/simple-neural-network/blob/main/Notebook/Simple_Neural_Network_Notebook.ipynb)"
      ],
      "metadata": {
        "id": "MdopClsH6_79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** Ian Jure Macalisang\n",
        "\n",
        "**Email:** ianjuremacalisang2@gmail.com\n",
        "\n",
        "**Link:** https://github.com/ianjure/simple-neural-network"
      ],
      "metadata": {
        "id": "hFfHOjb50n5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: UPLOAD **[mnist_train.csv](https://drive.google.com/file/d/1Mif0Xmvh4mubRNbmQrLOIJomFjOJNl5s/view)** FIRST BEFORE RUNNING THE 1ST CELL!"
      ],
      "metadata": {
        "id": "wiyuIPb31pcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMPORTING LIBRARIES AND THE DATA { display-mode: \"form\" }\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = pd.read_csv('mnist_train.csv')"
      ],
      "metadata": {
        "id": "vDhKx2zpDs2G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FORMAT TRAINING DATA { display-mode: \"form\" }\n",
        "\n",
        "def Format_Data(Dataframe):\n",
        "    \"\"\"\n",
        "    Convert whole data to array since we will be doing\n",
        "    matric calculations.\n",
        "    \"\"\"\n",
        "    train_data = np.array(Dataframe)\n",
        "    M, N = train_data.shape\n",
        "\n",
        "    \"\"\"\n",
        "    Transpose array so that the pixel values will be in columns not in rows,\n",
        "    because we want to make these values as input nodes.\n",
        "    \"\"\"\n",
        "    train_data_T = train_data.T\n",
        "\n",
        "    \"\"\"\n",
        "    Get the X (pixel values) and Y (True Labels)\n",
        "\n",
        "    N - The amount of training images we have\n",
        "    \"\"\"\n",
        "    Y = train_data_T[0]\n",
        "    X = train_data_T[1:N]\n",
        "\n",
        "    \"\"\"\n",
        "    Normalize pixel values from 0-255 (Integer) to 0-1 (Float) by dividing each value with 255.\n",
        "    In this way, the numbers will be small and the computation becomes easier and faster.\n",
        "    \"\"\"\n",
        "    X = X / 255.\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "yb1oA5OWFVNQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title INITIALIZE THE DATA { display-mode: \"form\" }\n",
        "\n",
        "def Initialize(In_Neuron, Hidden_Neuron, Out_Neuron):\n",
        "    \"\"\"\n",
        "    Initialize weights values between -0.5 to 0.5.\n",
        "    Initialize bias values to 0.\n",
        "\n",
        "    Arguments:\n",
        "    In_Neuron - Number of input neurons of the network\n",
        "    Hidden_Neuron - Number of hidden neurons of the network\n",
        "    Out_Neuron - Number of output neurons of the network\n",
        "    \"\"\"\n",
        "    # LAYER 1\n",
        "    W1 = np.random.uniform(-0.5, 0.5, (Hidden_Neuron, In_Neuron))\n",
        "    B1 = np.zeros((Hidden_Neuron, 1))\n",
        "\n",
        "    # LAYER 2\n",
        "    W2 = np.random.uniform(-0.5, 0.5, (Out_Neuron, Hidden_Neuron))\n",
        "    B2 = np.zeros((Out_Neuron, 1))\n",
        "\n",
        "    return W1, B1, W2, B2"
      ],
      "metadata": {
        "id": "qW4hBFl_fahv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DENSE LAYER { display-mode: \"form\" }\n",
        "\n",
        "def Dense(Weights, Bias, Input):\n",
        "    \"\"\"\n",
        "    The dense layer is a simple layer of neurons in which each\n",
        "    neuron receives input from all the neurons of the previous layer.\n",
        "\n",
        "    Z = W * X + B\n",
        "\n",
        "    Where:\n",
        "    Z - Output of the dense layer\n",
        "    W - Weights of the dense layer\n",
        "    X - Input to the dense layer\n",
        "    B - Bias of the dense layer\n",
        "    \"\"\"\n",
        "    Output = Weights @ Input + Bias\n",
        "    return Output"
      ],
      "metadata": {
        "id": "mlpnHkoHYsSY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ACTIVATION FUNCTIONS { display-mode: \"form\" }\n",
        "\n",
        "def Sigmoid(Input):\n",
        "    \"\"\"\n",
        "    Sigmoid = 1 / 1 + e^-z\n",
        "\n",
        "    Where:\n",
        "    z - Input to the activation function\n",
        "\n",
        "    Explanation:\n",
        "    Sigmoid equates all values of z to an output ranging between 0 and 1 thus creating an S-shaped graph.\n",
        "    This was a great step in neural networks as it allowed the outputs of these functions to be more in\n",
        "    line with standard probability situations.\n",
        "    \"\"\"\n",
        "    Output = 1 / (1 + np.exp(-Input))\n",
        "    return Output\n",
        "\n",
        "def Sigmoid_Prime(Input):\n",
        "    \"\"\"\n",
        "    Sigmoid' = Sigmoid(Z) * (1 - Sigmoid(Z))\n",
        "\n",
        "    Where:\n",
        "    Z - Input to the activation function\n",
        "\n",
        "    This is the derivative of the Sigmoid function that is\n",
        "    used to find the gradient of the activation function.\n",
        "    \"\"\"\n",
        "    Output = (Sigmoid(Input) * (1 - Sigmoid(Input)))\n",
        "    return Output\n",
        "\n",
        "def ReLU(Input):\n",
        "    \"\"\"\n",
        "    ReLU = Input if Input > 0\n",
        "    ReLU = 0 if Input <= 0\n",
        "    \"\"\"\n",
        "    Output = np.maximum(0, Input)\n",
        "    return Output\n",
        "\n",
        "def ReLU_Prime(Input):\n",
        "    \"\"\"\n",
        "    ReLU' = 1 if Input > 0\n",
        "    ReLU' = 0 if Input <= 0\n",
        "    \"\"\"\n",
        "    Output = np.where(Input > 0, 1, 0)\n",
        "    return Output"
      ],
      "metadata": {
        "id": "IRMTsbmgSNmS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LOSS FUNCTION { display-mode: \"form\" }\n",
        "\n",
        "def MSE(Y_Prediction, Y_True):\n",
        "    \"\"\"\n",
        "    Mean Squared Error (Batch) = 1 / N * ∑ (ŷ - Y)^2\n",
        "    Mean Squared Error (Single) = ∑ (ŷ - Y)^2\n",
        "\n",
        "    Where:\n",
        "    N - Amount of prediction or true labels passed\n",
        "    ŷ - Output prediction of the model (Predicted Y)\n",
        "    Y - True label of the training data (True Y)\n",
        "    \"\"\"\n",
        "    #Output = 1 / np.size(Y_True) * np.sum((Y_Prediction - Y_True) ** 2, axis=0)\n",
        "    Output = np.sum((Y_Prediction - Y_True) ** 2, axis=0)\n",
        "    return Output[0]\n",
        "\n",
        "def MSE_Prime(Y_Prediction, Y_True):\n",
        "    \"\"\"\n",
        "    MSE' (Batch) = 2 * ∑ (ŷ - Y)^2 / N\n",
        "    MSE' (Single) = ŷ - Y\n",
        "\n",
        "    Where:\n",
        "    N - Amount of prediction or true labels passed\n",
        "    ŷ - Output prediction of the model (Predicted Y)\n",
        "    Y - True label of the training data (True Y)\n",
        "    \"\"\"\n",
        "    #Output = 2 * (Y_Prediction - Y_True) / np.size(Y_True)\n",
        "    Output = Y_Prediction - Y_True\n",
        "    return Output"
      ],
      "metadata": {
        "id": "iRa3dfmW6K8B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HELPER FUNCTION { display-mode: \"form\" }\n",
        "\n",
        "def One_Hot_Encode(Label):\n",
        "    \"\"\"\n",
        "    One hot encoding is a technique that we use to represent categorical variables\n",
        "    as numerical values in a machine learning model. Mostly used in classification problems.\n",
        "\n",
        "    Example:\n",
        "    Label = 5\n",
        "    Output = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] - 5th Index is changed to 1 to signify that the\n",
        "    whole array is of category 5. TLDR; we converted a category into an array that\n",
        "    we can feed during the backpropagation process.\n",
        "\n",
        "    Argument:\n",
        "    Label - Array of the true labels\n",
        "\n",
        "    Returns:\n",
        "    ONE_HOT_Y - 2D array of encoded single digit labels\n",
        "    \"\"\"\n",
        "\n",
        "    # CREATE 2D ARRAY OF ZEROES WITH SHAPE: [n x m] n = number of data, m = number of categories\n",
        "    Output = np.zeros((Label.size, Label.max() + 1))\n",
        "\n",
        "    # CHANGE ARRAY[i, j] == 1 | Example: array[1,2] - change element into 1 where row is 1 and column is 2.\n",
        "    Output[np.arange(Label.size), Label] = 1\n",
        "\n",
        "    # TRANSPOSE 2D ARRAY\n",
        "    Output = Output.T\n",
        "\n",
        "    return Output"
      ],
      "metadata": {
        "id": "EeoCobOjTOeT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FORWARD PROPAGATION { display-mode: \"form\" }\n",
        "\n",
        "def Forward(X, W1, B1, W2, B2):\n",
        "    \"\"\"\n",
        "    FORWARD PROPAGATION\n",
        "\n",
        "    INPUT [ X ] -->  HIDDEN LAYER [ Z1 = W1 * X + B1 ]  -->  ACTIVATION FUNCTION [ A1 = Sigmoid(Z1) ]\n",
        "    -->  OUTPUT LAYER [ Z2 = W2 * A1 + B2 ]  --> ACTIVATION FUNCTION [ A2 = Sigmoid(Z2) ]  -->  PREDICTION!\n",
        "\n",
        "    Where:\n",
        "    X - Input data\n",
        "    W1 - Layer 1 weights\n",
        "    B1 - Layer 1 biases\n",
        "    W2 - Layer 2 weights\n",
        "    B2 - Layer 2 biases\n",
        "\n",
        "    Returns:\n",
        "    Z1 - Layer 1 result [ Z1 = W1 * X + B1 ]\n",
        "    A1 - Z1 result activated [ A1 = Sigmoid(Z1) ]\n",
        "    Z2 - Layer 2 result [ Z2 = W2 * A1 + B2 ]\n",
        "    A2 - Z2 result activated [ A2 = Sigmoid(Z2) ]\n",
        "    \"\"\"\n",
        "    # INPUT -> HIDDEN (LAYER 1)\n",
        "    Z1 = Dense(W1, B1, X) # DENSE LAYER\n",
        "    A1 = Sigmoid(Z1)  # ACTIVATION LAYER\n",
        "\n",
        "    # HIDDEN -> OUTPUT (LAYER 2)\n",
        "    Z2 = Dense(W2, B2, A1) # DENSE LAYER\n",
        "    A2 = Sigmoid(Z2) # ACTIVATION LAYER\n",
        "\n",
        "    return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "PQwO5rcchu4s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BACKWARD PROPAGATION { display-mode: \"form\" }\n",
        "\n",
        "def Backward(X, Y, Z1, A1, Z2, A2, W1, B1, W2, B2, LR):\n",
        "    \"\"\"\n",
        "    BACKWARD PROPAGATION\n",
        "\n",
        "    FIND THE DERIVATIVES WITH RESPECT TO THE PARAMETERS --> COMPUTE THE GRADIENTS\n",
        "    --> USE THE GRADIENTS TO UPDATE THE WEIGHTS AND BIASES VALUES!\n",
        "\n",
        "    A gradient simply measures the change in all weights with regard to the change in error.\n",
        "    You can also think of a gradient as the slope of a function.\n",
        "\n",
        "    Where:\n",
        "    X - Input data\n",
        "    Y - True label\n",
        "    Z1 - Layer 1 result [ Z1 = W1 * X + B1 ]\n",
        "    A1 - Z1 result activated [ A1 = Sigmoid(Z1) ]\n",
        "    Z2 - Layer 2 result [ Z2 = W2 * A1 + B2 ]\n",
        "    A2 - Z2 result activated [ A2 = Sigmoid(Z2) ]\n",
        "    W1 - Layer 1 weights\n",
        "    B1 - Layer 1 biases\n",
        "    W2 - Layer 2 weights\n",
        "    B2 - Layer 2 biases\n",
        "    LR - Learning Rate (Alpha)\n",
        "\n",
        "    Returns:\n",
        "    W1 - Updated layer 1 weights\n",
        "    B1 - Updated layer 1 biases\n",
        "    W2 - Updated layer 2 weights\n",
        "    B2 - Updated layer 2 biases\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    dE2 (GRADIENT OF LAYER 2)\n",
        "    = DERIVATIVE OF THE COST FUNCTION @ DERIVATIVE OF THE ACTIVATION FUNCTION 2\n",
        "\n",
        "    USE THIS TO FIND THE GRADIENT WITH RESPECT TO THE WEIGHTS (W2) AND BIAS (B2) OF LAYER 2\n",
        "    \"\"\"\n",
        "    dE2 = MSE_Prime(A2, LABEL) * Sigmoid_Prime(Z2)\n",
        "    dW2 = dE2 @ A1.T\n",
        "    dB2 = dE2\n",
        "\n",
        "    \"\"\"\n",
        "    dA1 (INTERMEDIATE GRADIENT)\n",
        "    = TRANSPOSE OF THE PARAMETER THAT AFFECTS A1 (W2) @ GRADIENT OF LAYER 2 (dE2)\n",
        "\n",
        "    USE THIS TO FIND THE GRADIENT OF LAYER 1\n",
        "    \"\"\"\n",
        "    dA1 = W2.T @ dE2\n",
        "\n",
        "    \"\"\"\n",
        "    dE2 (GRADIENT OF LAYER 1)\n",
        "    = INTERMEDIATE GRADIENT @ DERIVATIVE OF THE ACTIVATION FUNCTION 1\n",
        "\n",
        "    USE THIS TO FIND THE GRADIENT WITH RESPECT TO THE WEIGHTS (W1) AND BIAS (B1) OF LAYER 1\n",
        "    \"\"\"\n",
        "    dE1 = dA1 * Sigmoid_Prime(Z1)\n",
        "    dW1 = dE1 @ IMG.T\n",
        "    dB1 = dE1\n",
        "\n",
        "    \"\"\"\n",
        "    UPDATE WEIGHTS AND BIASES VALUES USING THE GRADIENTS\n",
        "\n",
        "    New Weights = Previous Weights - Alpha (Learning Rate) * Gradient of the Weights\n",
        "    New Bias = Previous Bias - Alpha (Learning Rate) * Gradient of the Bias\n",
        "    \"\"\"\n",
        "    W2 = W2 - LR * dW2\n",
        "    B2 = B2 - LR * dB2\n",
        "    W1 = W1 - LR * dW1\n",
        "    B1 = B1 - LR * dB1\n",
        "\n",
        "    return W2, B2, W1, B1"
      ],
      "metadata": {
        "id": "-h5p_ZNtlxNv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TRAINING{ display-mode: \"form\" }\n",
        "\n",
        "X_train, Y_train = Format_Data(data)\n",
        "images = X_train.T\n",
        "labels = One_Hot_Encode(Y_train).T\n",
        "\n",
        "# INITIALIZE WEIGHTS AND BIASES\n",
        "W1, B1, W2, B2 = Initialize(784, 20, 10)\n",
        "\n",
        "# TRACK ACCURACY AND LOSS\n",
        "accuracy = 0\n",
        "loss = 0\n",
        "\n",
        "# HYPERPARAMETER\n",
        "Alpha = 0.01 # @param {type:\"number\"}\n",
        "Epochs = 10 # @param {type:\"integer\"}\n",
        "\n",
        "\"\"\"\n",
        "SIMPLE NEURAL NETWORK\n",
        "\n",
        "INPUT -> HIDDEN LAYER [ DENSE LAYER -> ACTIVATION LAYER ]\n",
        "-> OUTPUT LAYER [ DENSE LAYER -> ACTIVATION LAYER ]\n",
        "\"\"\"\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "    for IMG, LABEL in zip(images, labels):\n",
        "\n",
        "        # ADD A DIMENSION TO IMG AND LABEL\n",
        "        IMG.shape += (1,) # (784,) -> (784, 1)\n",
        "        LABEL.shape += (1,) # (10,) -> (10, 1)\n",
        "\n",
        "        \"\"\"\n",
        "        GRADIENT DESCENT:\n",
        "        FORWARD PASS --> ERROR CALCULATION\n",
        "        --> BACKWARD PASS --> UPDATE WEIGHTS AND BIASES\n",
        "        \"\"\"\n",
        "\n",
        "        # FORWARD PASS\n",
        "        Z1, A1, Z2, A2 = Forward(IMG, W1, B1, W2, B2)\n",
        "\n",
        "        # LOSS AND ACCURACY CALCULATION\n",
        "        loss = MSE(A2, LABEL)\n",
        "        accuracy += int(np.argmax(A2) == np.argmax(LABEL))\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        W2, B2, W1, B1 = Backward(IMG, LABEL, Z1, A1, Z2, A2, W1, B1, W2, B2, Alpha)\n",
        "\n",
        "    # TRACK ACCURACY AND LOSS EVERY EPOCH\n",
        "    print(f\"Epoch {epoch} | Accuracy: {round((accuracy / images.shape[0]) * 100, 2)}% | Loss:\", \"{:.5f}\".format(loss))\n",
        "\n",
        "    # RESET VARIABLES\n",
        "    accuracy = 0\n",
        "    loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmcw2MTRLDJR",
        "outputId": "ade111fc-53bf-4733-c23f-7dc1d22494a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Accuracy: 65.03% | Loss: 0.51114\n",
            "Epoch 1 | Accuracy: 86.38% | Loss: 0.19578\n",
            "Epoch 2 | Accuracy: 89.13% | Loss: 0.11214\n",
            "Epoch 3 | Accuracy: 90.32% | Loss: 0.08087\n",
            "Epoch 4 | Accuracy: 91.05% | Loss: 0.06381\n",
            "Epoch 5 | Accuracy: 91.5% | Loss: 0.05191\n",
            "Epoch 6 | Accuracy: 91.84% | Loss: 0.04284\n",
            "Epoch 7 | Accuracy: 92.12% | Loss: 0.03568\n",
            "Epoch 8 | Accuracy: 92.42% | Loss: 0.03001\n",
            "Epoch 9 | Accuracy: 92.68% | Loss: 0.02552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TEST THE MODEL { display-mode: \"form\" }\n",
        "\n",
        "index = random.randint(0, images.shape[0]-1)\n",
        "img = images[index]\n",
        "plt.imshow(img.reshape(28, 28), cmap=\"Greys\")\n",
        "img.shape += (1,)\n",
        "\n",
        "# FORWARD PASS FOR TESTING\n",
        "Z1 = np.matmul(W1, img) + B1\n",
        "A1 = Sigmoid(Z1)\n",
        "Z2 = np.matmul(W2, A1) + B2\n",
        "A2 = Sigmoid(Z2)\n",
        "\n",
        "# PREDICTION\n",
        "plt.title(f\"PREDICTION: {A2.argmax()}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NIpbmqxlMyFV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "8dee6b80-389b-44ea-998f-955ec910fce0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\mathrm{THE}$ $\\mathrm{MATHEMATICS}$ $\\mathrm{BEHIND}$"
      ],
      "metadata": {
        "id": "Kg2_S9afejbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\mathrm{FORWARD-PROPAGATION}$\n",
        "\n",
        "$Z1 = W1 * A0 + b1$\n",
        "\n",
        "$A1 = act^1(Z1)$\n",
        "\n",
        "$Z2 = W2 * A1 + b2$\n",
        "\n",
        "$A2 = act^2(Z2)$\n",
        "\n",
        "<br />\n",
        "\n",
        "$where,$\n",
        "\n",
        "$A0$ - Data Input\n",
        "\n",
        "$W1$ - Weights of Layer 1\n",
        "\n",
        "$b1$ - Bias of Layer 1\n",
        "\n",
        "$W2$ - Weights of Layer 2\n",
        "\n",
        "$b2$ - Bias of Layer 2\n",
        "\n",
        "$act^1$ - Activation Function of Layer 1\n",
        "\n",
        "$act^2$ - Activation Function of Layer 2"
      ],
      "metadata": {
        "id": "ZPTvNNGGfvpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\mathrm{BACKWARD-PROPAGATION}$\n",
        "\n",
        "</br>\n",
        "\n",
        "## **COMPUTING THE GRADIENTS**\n",
        "\n",
        "$dE2 = \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} = (A2 - \\hat{y}) \\cdot act^2\\prime(z2)$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$A2$ - Output of the Network\n",
        "\n",
        "$\\hat{y}$ - True Label\n",
        "\n",
        "$act^2\\prime$ - Derivative of the Activation Function of Layer 2\n",
        "\n",
        "<br />\n",
        "\n",
        "$dW2 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial W2} = dE2 \\cdot \\frac{\\partial Z2}{\\partial W2} = dE2 \\cdot A1.T$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\frac{\\partial Z2}{\\partial W2} = \\frac{\\partial(W2*A1+b2)}{\\partial W2} = A1$\n",
        "\n",
        "$T$ - Matrix Transposed\n",
        "\n",
        "<br />\n",
        "\n",
        "$db2 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial b2} = dE2 \\cdot \\frac{\\partial Z2}{\\partial b2} = dE2 \\cdot 1$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\frac{\\partial Z2}{\\partial b2} = \\frac{\\partial(W2*A1+b2)}{\\partial b2} = 1$\n",
        "\n",
        "<br />\n",
        "\n",
        "$dA1 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1} = dE2 \\cdot \\frac{\\partial Z2}{\\partial A1} = dE2 \\cdot W2.T$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\frac{\\partial Z2}{\\partial A1} = \\frac{\\partial(W2*A1+b2)}{\\partial A1} = W2$\n",
        "\n",
        "$T$ - Matrix Transposed\n",
        "\n",
        "<br />\n",
        "\n",
        "$dE1 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1} \\cdot \\frac{\\partial A1}{\\partial Z1}= dA1 \\cdot \\frac{\\partial A1}{\\partial Z1} = dA1 \\cdot act^1\\prime(z1)$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$act^1\\prime$ - Derivative of the Activation Function of Layer 1\n",
        "\n",
        "<br />\n",
        "\n",
        "$dW1 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1} \\cdot \\frac{\\partial A1}{\\partial Z1} \\cdot \\frac{\\partial Z1}{\\partial W1} = dE1 \\cdot \\frac{\\partial Z1}{\\partial W1} = dE1 \\cdot A0.T$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\frac{\\partial Z1}{\\partial W1} = \\frac{\\partial(W1*A0+b1)}{\\partial W1} = A0$\n",
        "\n",
        "$A0$ - Data Input\n",
        "\n",
        "$T$ - Matrix Transposed\n",
        "\n",
        "<br />\n",
        "\n",
        "$db1 =  \\frac{\\partial C}{\\partial A2} \\cdot \\frac{\\partial A2}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1} \\cdot \\frac{\\partial A1}{\\partial Z1} \\cdot \\frac{\\partial Z1}{\\partial b1} = dE1 \\cdot \\frac{\\partial Z1}{\\partial b1} = dE1 \\cdot 1$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\frac{\\partial Z1}{\\partial b1} = \\frac{\\partial(W1*A0+b1)}{\\partial b1} = 1$\n",
        "\n",
        "<br />\n",
        "\n",
        "## **UPDATING THE WEIGHTS AND BIASES**\n",
        "\n",
        "$W1 = W1 - \\alpha * dW1$\n",
        "\n",
        "$b1 = b1 - \\alpha * db1$\n",
        "\n",
        "$W2 = W2 - \\alpha * dW2$\n",
        "\n",
        "$b2 = b2 - \\alpha * db2$\n",
        "\n",
        "$where,$\n",
        "\n",
        "$\\alpha$ - Learning Rate\n",
        "\n",
        "$dW1, db1, dW2, db2$ - Gradients"
      ],
      "metadata": {
        "id": "PKZqajZanyTN"
      }
    }
  ]
}
